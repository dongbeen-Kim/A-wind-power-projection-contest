from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")

import os
import polars as pl
from concurrent.futures import ThreadPoolExecutor, as_completed
import datetime as dt
from datetime import datetime
from tqdm import tqdm
from typing import List, Dict, Optional
from dataclasses import dataclass

import xgboost as xgb
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import Lasso
import numpy as np
import pandas as pd

@dataclass
class LocationConfig:
    name: str
    lat: float
    lon: float
    lat_range: float
    lon_range: float
    start_date: str
    end_date: str


class WindPowerPipeline:
    def __init__(self, base_path: str, max_workers: int = 8):
        """
        í’ë ¥ ë°œì „ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            base_path (str): ê¸°ë³¸ ì‘ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_workers (int): ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ ìˆ˜
        """
        self.base_path = Path(base_path)
        self.max_workers = max_workers
        self.schema_map = {}

        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.ldaps_path = self.base_path / "ldaps"
        self.output_path = self.base_path / "output"
        self.target_path = self.base_path / "target"
        self.result_path = self.base_path / "results"
        self.derived_data_path = self.base_path / "íŒŒìƒë³€ìˆ˜ì¶”ê°€ëœë°ì´í„°"

        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.result_path.mkdir(exist_ok=True)

    def _get_location_configs(self) -> Dict[str, LocationConfig]:
        """ì§€ì—­ë³„ ì„¤ì • ì •ë³´ ë°˜í™˜"""
        return {
            'gyeongju': LocationConfig(
                name='ê²½ì£¼',
                lat=35.7149,
                lon=129.3693,
                lat_range=0.015,
                lon_range=0.015,
                start_date='2020-01-01',
                end_date='2024-12-31'
            ),
            'yangyang': LocationConfig(
                name='ì–‘ì–‘',
                lat=37.9330943,
                lon=128.6943946,
                lat_range=0.02,
                lon_range=0.02,
                start_date='2024-04-01',
                end_date='2025-03-31'
            ),
            'yeongdeok': LocationConfig(
                name='ì˜ë•',
                lat=36.4198685,
                lon=129.3960048,
                lat_range=0.015,
                lon_range=0.015,
                start_date='2024-04-01',
                end_date='2025-03-31'
            )
        }

    def _date_in_range(self, date_str: str, start: str, end: str) -> bool:
        """ë‚ ì§œê°€ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
        date = dt.datetime.strptime(date_str, "%Y%m%d").date()
        return dt.date.fromisoformat(start) <= date <= dt.date.fromisoformat(end)

    def _filter_lat_lon_lazy(self, df: pl.LazyFrame, config: LocationConfig) -> pl.LazyFrame:
        """ìœ„ë„/ê²½ë„ í•„í„°ë§"""
        lat_min = config.lat - config.lat_range
        lat_max = config.lat + config.lat_range
        lon_min = config.lon - config.lon_range
        lon_max = config.lon + config.lon_range

        return df.filter(
            (pl.col("latitude") >= lat_min) & (pl.col("latitude") <= lat_max) &
            (pl.col("longitude") >= lon_min) & (pl.col("longitude") <= lon_max)
        )

    def _get_target_dates(self, config: LocationConfig) -> List[str]:
        """ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡ ë°˜í™˜"""
        root_folder = str(self.ldaps_path)
        output_folder = str(self.output_path)

        all_dates = [d for d in os.listdir(root_folder)
                     if os.path.isdir(os.path.join(root_folder, d)) and d.isdigit()]
        done_dates = [d for d in os.listdir(output_folder)
                      if os.path.isdir(os.path.join(output_folder, d)) and d.isdigit()] if os.path.exists(
            output_folder) else []
        target_dates = sorted([d for d in all_dates if d not in done_dates])

        return [d for d in target_dates
                if self._date_in_range(d, config.start_date, config.end_date)]

    def process_weather_data(self):
        """ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬"""
        print("=== ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

        processor = WeatherDataProcessor(max_workers=self.max_workers)
        locations = ['gyeongju', 'yangyang', 'yeongdeok']

        for location in locations:
            try:
                config = processor._get_location_configs()[location]
                location_result_path = self.result_path / f"{config.name}_test"
                location_result_path.mkdir(exist_ok=True)

                parquet_path = location_result_path / f"{config.name}_timeseries_MAX.parquet"
                csv_path = location_result_path / f"{config.name}_timeseries_MAX.csv"

                processor.process_location_data(
                    location_key=location,
                    root_folder=str(self.ldaps_path),
                    output_folder=str(self.output_path),
                    output_parquet=str(parquet_path),
                    output_csv=str(csv_path)
                )

            except Exception as e:
                print(f"{location} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def load_weather_data(self):
        """ì›ë³¸ ë‚ ì”¨ ë°ì´í„° ë¡œë“œ (MAX ë°ì´í„°)"""
        print("=== ë‚ ì”¨ ë°ì´í„° ë¡œë“œ ===")

        # ë¨¼ì € ìƒˆë¡œ ìƒì„±ëœ MAX ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        max_files = {
            'gyeongju': self.result_path / "ê²½ì£¼_test/ê²½ì£¼_timeseries_MAX.parquet",
            'yangyang': self.result_path / "ì–‘ì–‘_test/ì–‘ì–‘_timeseries_MAX.parquet",
            'yeongdeok': self.result_path / "ì˜ë•_test/ì˜ë•_timeseries_MAX.parquet"
        }

        # MAX íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        all_max_exist = all(path.exists() for path in max_files.values())

        if all_max_exist:
            print("ìƒì„±ëœ MAX ë°ì´í„° ì‚¬ìš©")
            df1 = pd.read_parquet(max_files['gyeongju'])
            df2 = pd.read_parquet(max_files['yangyang'])
            df3 = pd.read_parquet(max_files['yeongdeok'])
            print("MAX ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        else:
            print("MAX ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            raise FileNotFoundError("MAX ì‹œê³„ì—´ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return df1, df2, df3

    def load_target_data(self):
        """íƒ€ê²Ÿ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("=== íƒ€ê²Ÿ ë°ì´í„° ì²˜ë¦¬ ===")

        dataframes = []
        target_files = list(self.target_path.glob('*.parquet'))

        if not target_files:
            raise FileNotFoundError(f"íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.target_path}")

        for file in target_files:
            df = pd.read_parquet(file)
            filename_suffix = file.stem.split('_')[-1]
            df['filename'] = filename_suffix
            dataframes.append(df)

        if dataframes:
            merged_df = pd.concat(dataframes, ignore_index=True)
        else:
            merged_df = pd.DataFrame()

        merged_df.drop(['êµ¬ë¶„', 'ì‹œê°„', 'energy_mwh', 'plant_name', 'period_hours'],
                       axis=1, inplace=True, errors='ignore')
        merged_df['date'] = merged_df['end_datetime'].dt.date
        merged_df['hour'] = merged_df['end_datetime'].dt.hour

        # íƒ€ê²Ÿ ë°ì´í„° ì €ì¥
        target_output_path = self.result_path / 'target.parquet'
        merged_df.to_parquet(target_output_path, engine='pyarrow', index=False)

        return merged_df

    def create_features(self, df1, df2, df3, target_data):
        """íŒŒìƒë³€ìˆ˜ ìƒì„± ë° ë°ì´í„° ê²°í•©"""
        print("=== íŒŒìƒë³€ìˆ˜ ìƒì„± ë° ë°ì´í„° ê²°í•© ===")

        # íƒ€ê²Ÿ ë°ì´í„°ì™€ ë‚ ì”¨ ë°ì´í„° ê²°í•©
        target_data['end_datetime'] = target_data['end_datetime'].dt.tz_localize(None)
        df1['time'] = pd.to_datetime(df1['time'])
        df2['time'] = pd.to_datetime(df2['time'])
        df3['time'] = pd.to_datetime(df3['time'])

        # ê° ì§€ì—­ë³„ ë°ì´í„° ê²°í•©
        gy = pd.merge(df1, target_data[target_data['filename'] == 'gyeongju'],
                      left_on='time', right_on='end_datetime', how='left')
        gy.drop(['end_datetime', 'filename', 'date', 'hour'], axis=1, inplace=True, errors='ignore')

        yd = pd.merge(df3, target_data[target_data['filename'] == 'yeongduk'],
                      left_on='time', right_on='end_datetime', how='left')
        yd.drop(['end_datetime', 'filename', 'date', 'hour'], axis=1, inplace=True, errors='ignore')

        yy = pd.merge(df2, target_data[target_data['filename'] == 'yangyang'],
                      left_on='time', right_on='end_datetime', how='left')
        yy.drop(['end_datetime', 'filename', 'date', 'hour'], axis=1, inplace=True, errors='ignore')

        # íŒŒìƒë³€ìˆ˜ ìƒì„±
        def calculate_absolute_humidity_vec(temp_K, humidity):
            temp_C = temp_K - 273.15
            e_s = 6.112 * np.exp((17.67 * temp_C) / (temp_C + 243.5))
            e = humidity * e_s / 100
            AH = (216.7 * e) / temp_K
            return AH

        def add_derived_features(df, turbine_rad):
            # ì ˆëŒ€ìŠµë„
            df['absolute_humidity'] = calculate_absolute_humidity_vec(df['ta_1p5m'], df['rh_1p5m'])

            # í„°ë¹ˆ ë©´ì 
            df['turbine_area'] = np.pi * turbine_rad ** 2

            # ëŒí’ì†ë„
            df['storm_speed'] = np.sqrt(df['fvmax_50m'] ** 2 + df['fvmin_50m'] ** 2)

            # ê³µê¸° ë°€ë„
            R = 287.05
            df['air_density'] = (df['pmsl'] * 100) / (R * df['ta_1p5m'])

            # í’ì†
            df['wind_speed'] = np.sqrt(df['uws_10m'] ** 2 + df['vws_10m'] ** 2)
            df['wind_speed_squared'] = df['wind_speed'] ** 2

            # í’í–¥
            df['wind_direction'] = (np.degrees(np.arctan2(df['vws_10m'], df['uws_10m'])) + 360) % 360
            df['storm_direction'] = (np.degrees(np.arctan2(df['fvmin_50m'], df['fvmax_50m'])) + 360) % 360
            df['wind_direction_diff'] = abs(df['wind_direction'] - df['storm_direction'])

            # í’ì†-í’í–¥ ìƒí˜¸ì‘ìš©
            df['wind_speed_direction_interaction'] = df['wind_speed'] * np.cos(np.radians(df['wind_direction']))

            # ë‚œë¥˜ ê°•ë„
            df['turbulence_intensity'] = (df['fvmax_50m'] - df['fvmin_50m']) / (df['wind_speed'] + 0.1)

            # í’ë ¥ ì—ë„ˆì§€ ê³„ì‚°
            df['wind_energy'] = 0.5 * df['air_density'] * df['turbine_area'] * df['wind_speed'] ** 3 / 1000 / 3600

            # ë‚ ì§œ ê´€ë ¨ íŒŒìƒë³€ìˆ˜
            df['year'] = df['time'].dt.year
            df['month'] = df['time'].dt.month
            df['day'] = df['time'].dt.day
            df['hour'] = df['time'].dt.hour

            return df

        # ê° ì§€ì—­ë³„ íŒŒìƒë³€ìˆ˜ ìƒì„±
        turbine_rad_gy = 56.5
        turbine_rad_yd = 75.5
        turbine_rad_yy = 67.85

        gy = add_derived_features(gy, turbine_rad_gy)
        yd = add_derived_features(yd, turbine_rad_yd)
        yy = add_derived_features(yy, turbine_rad_yy)

        return gy, yd, yy

    def preprocess_data(self, gy, yd, yy):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        print("=== ë°ì´í„° ì „ì²˜ë¦¬ ===")

        # ê²½ì£¼ ë°ì´í„° ì „ì²˜ë¦¬
        gy = gy[gy['time'] != '2020-01-01 00:00:00']
        gy = gy[~gy.drop(['energy_kwh'], axis=1).isna().any(axis=1)]

        # ì˜ë• ë°ì´í„° ì „ì²˜ë¦¬ (ë³´ê°„)
        yd = yd.set_index('time', drop=False)
        full_date_range = pd.date_range(start="2024-04-01 00:00:00", end="2025-03-31 23:00:00", freq='H')
        yd = yd.reindex(full_date_range)
        columns_to_interpolate = [col for col in yd.columns if col not in ['energy_kwh', 'time']]
        yd[columns_to_interpolate] = yd[columns_to_interpolate].interpolate(method='linear')

        # ì–‘ì–‘ ë°ì´í„° ì „ì²˜ë¦¬ (ë³´ê°„)
        yy = yy[yy['time'] != '2024-04-01 00:00:00']
        yy = yy.set_index('time', drop=False)
        full_date_range = pd.date_range(start="2024-04-01 01:00:00", end="2025-03-31 23:00:00", freq='H')
        yy = yy.reindex(full_date_range)
        columns_to_interpolate = [col for col in yy.columns if col not in ['energy_kwh', 'time']]
        yy[columns_to_interpolate] = yy[columns_to_interpolate].interpolate(method='linear')

        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        gy.to_parquet(self.result_path / 'mid_output_gy.parquet')
        yy.to_parquet(self.result_path / 'mid_output_yy.parquet')
        yd.to_parquet(self.result_path / 'mid_output_yd.parquet')

        print("ì „ì²˜ë¦¬ ì™„ë£Œ")
        return gy, yd, yy

    def train_models_and_predict(self, gy, yd, yy):
        """ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ê°€ì¤‘ì¹˜ ì†ì‹¤ + ì •ì‚°ê¸ˆ ìµœì í™” + ì‹œê°„ëŒ€ ë¶„ë¦¬ ëª¨ë¸)"""
        print("=== ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ê°œì„  í¬ì¸íŠ¸ 1,2,3 ì ìš©) ===")

        from sklearn.model_selection import RandomizedSearchCV
        from scipy.stats import randint, uniform
        import numpy as np
        from functools import partial

        # ë°œì „ì†Œ ìš©ëŸ‰ì„ ë°ì´í„°ì—ì„œ ìë™ ì¶”ì •
        CAPACITY = {
            'ê²½ì£¼í’ë ¥': gy['energy_kwh'].max() * 1.04,
            'ì˜ë•í’ë ¥': yd['energy_kwh'].max() * 1.05,
            'ì–‘ì–‘í’ë ¥': yy['energy_kwh'].max() * 1.05
        }

        print(f"ì¶”ì • ìš©ëŸ‰: {CAPACITY}")

        def custom_weighted_loss(y_true, y_pred, capacity):
            """ì¤‘ìš” ì‹œê°„ëŒ€ ê°€ì¤‘ì¹˜ ì ìš© ì†ì‹¤ í•¨ìˆ˜"""
            errors = np.abs(y_true - y_pred)
            weights = np.where(y_true >= 0.1 * capacity, 2.0, 1.0)
            return np.mean(weights * errors)

        def bias_correction(y_pred, y_train, capacity):
            """ì •ì‚°ê¸ˆ ìµœì í™”ë¥¼ ìœ„í•œ ë°”ì´ì–´ìŠ¤ ë³´ì •"""
            mean_pred = np.mean(y_pred)
            mean_actual = np.mean(y_train)
            bias = mean_actual - mean_pred

            # y_corrected = y_pred + bias * 0.5
            y_corrected = y_pred
            high_generation_mask = y_corrected >= 0.1 * capacity
            y_corrected[high_generation_mask] *= 1.05
            y_corrected = np.clip(y_corrected, 0, capacity)

            return y_corrected
        # âœ… 5ï¸âƒ£ ì¶”ê°€: ì˜ˆì¸¡ smoothing (í›„ì²˜ë¦¬)
        def smooth_predictions(y_pred, window=3):
            """
            ê¸‰ê²©í•œ ë³€í™”ë¥¼ ì™„í™”í•˜ëŠ” ì˜ˆì¸¡ smoothing í•¨ìˆ˜
            ì´ë™í‰ê·  í•„í„°ë¡œ ì˜ˆì¸¡ ë³€ë™ì„ ì™„í™”í•˜ì—¬ MAE ì•ˆì •í™”
            """
            if len(y_pred) < window:
                return y_pred
            return np.convolve(y_pred, np.ones(window)/window, mode='same')

        def custom_scorer(estimator, X, y_true, capacity):
            """ì»¤ìŠ¤í…€ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜"""
            y_pred = estimator.predict(X)
            return -custom_weighted_loss(y_true, y_pred, capacity)

        def train_dual_model(X_train, y_train, X_test, capacity, model_type='rf'):
            """
            ì¤‘ìš” ì‹œê°„ëŒ€ì™€ ì¼ë°˜ ì‹œê°„ëŒ€ë¥¼ ë¶„ë¦¬í•˜ì—¬ í•™ìŠµí•˜ëŠ” ì´ì¤‘ ëª¨ë¸
            """
            threshold = 0.1 * capacity

            # numpy arrayë¡œ ë³€í™˜ (indexingì„ ìœ„í•´)
            y_train_arr = y_train.values if hasattr(y_train, 'values') else y_train

            # ì¤‘ìš” ì‹œê°„ëŒ€ (ë°œì „ëŸ‰ >= 10%)
            high_mask_train = y_train_arr >= threshold
            X_train_high = X_train[high_mask_train]
            y_train_high = y_train_arr[high_mask_train]

            # ì¼ë°˜ ì‹œê°„ëŒ€ (ë°œì „ëŸ‰ < 10%)
            low_mask_train = y_train_arr < threshold
            X_train_low = X_train[low_mask_train]
            y_train_low = y_train_arr[low_mask_train]

            print(f"  ì¤‘ìš” ì‹œê°„ëŒ€ ìƒ˜í”Œ: {len(y_train_high)} / ì¼ë°˜ ì‹œê°„ëŒ€ ìƒ˜í”Œ: {len(y_train_low)}")

            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
            if model_type == 'rf':
                high_params = {
                    'n_estimators': randint(300, 700),
                    'max_depth': [20, 25, 30, None],
                    'min_samples_split': randint(2, 10),
                    'min_samples_leaf': randint(1, 5),
                    'max_features': ['sqrt', 'log2']
                }
                low_params = {
                    'n_estimators': randint(100, 400),
                    'max_depth': [10, 15, 20, None],
                    'min_samples_split': randint(2, 15),
                    'min_samples_leaf': randint(1, 8),
                    'max_features': ['sqrt', 'log2']
                }
                base_model_high = RandomForestRegressor(random_state=42, n_jobs=-1)
                base_model_low = RandomForestRegressor(random_state=42, n_jobs=-1)
            elif model_type == 'lgbm':
                high_params = {
                    'n_estimators': randint(300, 700),
                    'max_depth': randint(8, 20),
                    'learning_rate': uniform(0.01, 0.15),
                    'num_leaves': randint(50, 150),
                    'min_child_samples': randint(5, 50),
                    'subsample': uniform(0.75, 0.25),
                    'colsample_bytree': uniform(0.75, 0.25)
                }
                low_params = {
                    'n_estimators': randint(100, 400),
                    'max_depth': randint(5, 15),
                    'learning_rate': uniform(0.01, 0.2),
                    'num_leaves': randint(20, 100),
                    'min_child_samples': randint(10, 80),
                    'subsample': uniform(0.7, 0.3),
                    'colsample_bytree': uniform(0.7, 0.3)
                }
                base_model_high = LGBMRegressor(random_state=42, verbose=-1)
                base_model_low = LGBMRegressor(random_state=42, verbose=-1)
            # ğŸ’¡ XGBoost ì¶”ê°€
            elif model_type == 'xgb':
                high_params = {
                    'n_estimators': randint(300, 700),
                    'max_depth': randint(8, 20),
                    'learning_rate': uniform(0.01, 0.15),
                    'subsample': uniform(0.75, 0.25),
                    'colsample_bytree': uniform(0.75, 0.25)
                }
                low_params = {
                    'n_estimators': randint(100, 400),
                    'max_depth': randint(5, 15),
                    'learning_rate': uniform(0.01, 0.2),
                    'subsample': uniform(0.7, 0.3),
                    'colsample_bytree': uniform(0.7, 0.3)
                }
                base_model_high = xgb.XGBRegressor(random_state=42, n_jobs=-1)
                base_model_low = xgb.XGBRegressor(random_state=42, n_jobs=-1)

            # ğŸ’¡ SVR ì¶”ê°€ (íŒŒë¼ë¯¸í„° ê³µê°„ì€ ì¢ê²Œ ì„¤ì •í•˜ê±°ë‚˜, Grid Search ê³ ë ¤)
            elif model_type == 'svr':
                high_params = {
                    'C': uniform(0.5, 5.0), # ê·œì œ íŒŒë¼ë¯¸í„°
                    'gamma': ['scale', 'auto', uniform(0.001, 0.1)], # ì»¤ë„ ê³„ìˆ˜
                    'epsilon': uniform(0.01, 0.5) # í—ˆìš© ì˜¤ì°¨
                }
                low_params = {
                    'C': uniform(0.1, 2.0),
                    'gamma': ['scale', 'auto', uniform(0.0001, 0.01)],
                    'epsilon': uniform(0.05, 0.2)
                }
                # SVRì€ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ í•™ìŠµ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                base_model_high = SVR(kernel='rbf')
                base_model_low = SVR(kernel='rbf')

            # ğŸ’¡ Ridge ì¶”ê°€ (ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì„ í˜• ëª¨ë¸)
            elif model_type == 'ridge':
                high_params = {
                    'alpha': uniform(0.1, 10.0), # ê·œì œ ê°•ë„
                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']
                }
                low_params = {
                    'alpha': uniform(0.01, 5.0),
                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']
                }
                # RidgeëŠ” n_jobsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì˜
                base_model_high = Ridge(random_state=42)
                base_model_low = Ridge(random_state=42)

            # ğŸ’¡ MLPRegressor ì¶”ê°€ (ì¸ê³µì‹ ê²½ë§)
            elif model_type == 'mlp':
                high_params = {
                    'hidden_layer_sizes': [(randint(50, 200).rvs(),), (randint(50, 150).rvs(), randint(20, 80).rvs())],
                    'activation': ['relu', 'tanh'],
                    'solver': ['adam'],
                    'learning_rate_init': uniform(0.0001, 0.01),
                    'max_iter': randint(300, 800)
                }
                low_params = {
                    'hidden_layer_sizes': [(randint(20, 100).rvs(),)],
                    'activation': ['relu', 'tanh'],
                    'solver': ['adam'],
                    'learning_rate_init': uniform(0.001, 0.05),
                    'max_iter': randint(100, 500)
                }
                base_model_high = MLPRegressor(random_state=42)
                base_model_low = MLPRegressor(random_state=42)

            # ğŸ’¡ ElasticNet ì¶”ê°€ (L1, L2 ê·œì œê°€ í˜¼í•©ëœ ì„ í˜• ëª¨ë¸)
            elif model_type == 'elasticnet':
                high_params = {
                    'alpha': uniform(0.001, 50.0), # ì „ì²´ ê·œì œ ê°•ë„
                    'l1_ratio': uniform(0.0, 1.0), # L1 ê·œì œì˜ í˜¼í•© ë¹„ìœ¨ (0: L2, 1: L1)
                    'selection': ['cyclic', 'random']
                }
                low_params = {
                    'alpha': uniform(0.001, 20.0),
                    'l1_ratio': uniform(0.0, 1.0),
                    'selection': ['cyclic', 'random']
                }
                # ElasticNetì€ n_jobsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì˜
                base_model_high = ElasticNet(random_state=42)
                base_model_low = ElasticNet(random_state=42)

            # ğŸ’¡ Lasso ì¶”ê°€ (L1 ê·œì œë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ í˜• ëª¨ë¸ - íŠ¹ì„± ì„ íƒ íš¨ê³¼)
            elif model_type == 'lasso':
                high_params = {
                    'alpha': uniform(0.1, 10.0), # ê·œì œ ê°•ë„
                    'selection': ['cyclic', 'random']
                }
                low_params = {
                    'alpha': uniform(0.01, 5.0),
                    'selection': ['cyclic', 'random']
                }
                # LassoëŠ” n_jobsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì˜
                base_model_high = Lasso(random_state=42)
                base_model_low = Lasso(random_state=42)

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")

            # ì¤‘ìš” ì‹œê°„ëŒ€ ëª¨ë¸ í•™ìŠµ
            print("  ì¤‘ìš” ì‹œê°„ëŒ€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
            search_high = RandomizedSearchCV(
                base_model_high,
                high_params,
                n_iter=20,
                cv=3,
                scoring='neg_mean_absolute_error',
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            search_high.fit(X_train_high, y_train_high)
            print(f"  ì¤‘ìš” ì‹œê°„ëŒ€ ìµœì  ì ìˆ˜: {-search_high.best_score_:.2f}")

            # ì¼ë°˜ ì‹œê°„ëŒ€ ëª¨ë¸ í•™ìŠµ
            print("  ì¼ë°˜ ì‹œê°„ëŒ€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
            search_low = RandomizedSearchCV(
            base_model_low,
            low_params,
            n_iter=15 if model_type not in ['svr', 'mlp', 'ridge'] else 7, # SVR, MLPëŠ” íŠœë‹ íšŸìˆ˜ ì¤„ì„
            cv=3,
            scoring='neg_mean_absolute_error',
            random_state=42,
            n_jobs=-1 if model_type not in ['svr', 'ridge', 'mlp'] else 1, # n_jobs ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì²˜ë¦¬
            verbose=0
            )
            search_low.fit(X_train_low, y_train_low)
            print(f"  ì¼ë°˜ ì‹œê°„ëŒ€ ìµœì  ì ìˆ˜: {-search_low.best_score_:.2f}")

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
            y_pred_high = search_high.best_estimator_.predict(X_test)
            y_pred_low = search_low.best_estimator_.predict(X_test)

            # ì¤‘ìš” ì‹œê°„ëŒ€ ì˜ˆì¸¡ê°’ì€ ë†’ì€ ëª¨ë¸, ë‚®ì€ ì˜ˆì¸¡ê°’ì€ ë‚®ì€ ëª¨ë¸ ì‚¬ìš©
            y_pred_final = np.where(y_pred_high >= threshold, y_pred_high, y_pred_low)

            return y_pred_final, search_high.best_estimator_, search_low.best_estimator_

        results_list = []

        # ========== 1. ê²½ì£¼ ëª¨ë¸ (ë¶„ë¦¬ ëª¨ë¸ ì ìš©) ==========
        print("\nê²½ì£¼ ëª¨ë¸ í•™ìŠµ ì¤‘ (ë¶„ë¦¬ ëª¨ë¸)...")
        train_data = gy[gy['year'] <= 2023]
        test_data = gy[gy['year'] == 2024]

        features = [col for col in gy.columns if col not in ['time', 'energy_kwh']]

        X_train = train_data[features]  # DataFrame ìœ ì§€
        y_train = train_data['energy_kwh']  # Series ìœ ì§€
        X_test = test_data[features]  # DataFrame ìœ ì§€

        capacity_gy = CAPACITY['ê²½ì£¼í’ë ¥']

        # ë¶„ë¦¬ ëª¨ë¸ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡
        predict_energy_kwh, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_gy, model_type='rf')
        # predict_energy_kwh, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_gy, model_type='lgbm')

        # ë°”ì´ì–´ìŠ¤ ë³´ì •
        predict_energy_kwh = bias_correction(predict_energy_kwh, y_train.values, capacity_gy)

        # âœ… ì˜ˆì¸¡ í›„ smoothing ì ìš©
        predict_energy_kwh = smooth_predictions(predict_energy_kwh, window=5)
        result_gy = pd.DataFrame({
            'time': test_data['time'].values,
            'energy_kwh': predict_energy_kwh,
            'plant_name': 'ê²½ì£¼í’ë ¥'
        })
        results_list.append(result_gy)
        print("ê²½ì£¼ ëª¨ë¸ ì™„ë£Œ")

        # ========== 2. ì˜ë• ëª¨ë¸ (ë¶„ë¦¬ ëª¨ë¸ ì ìš©) ==========
        print("\nì˜ë• ëª¨ë¸ í•™ìŠµ ì¤‘ (ë¶„ë¦¬ ëª¨ë¸)...")
        train_mask = yd['month'] % 2 == 1
        test_mask = yd['month'] % 2 == 0

        X_train = yd[train_mask].drop(['time', 'energy_kwh'], axis=1)  # DataFrame ìœ ì§€
        X_test = yd[test_mask].drop(['time', 'energy_kwh'], axis=1)  # DataFrame ìœ ì§€
        y_train = yd.loc[train_mask, 'energy_kwh']  # Series ìœ ì§€

        capacity_yd = CAPACITY['ì˜ë•í’ë ¥']

        # ë¶„ë¦¬ ëª¨ë¸ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡
        y_pred_yd, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_yd, model_type='elasticnet')
        # y_pred_yd, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_yd, model_type='lgbm')

        # ë°”ì´ì–´ìŠ¤ ë³´ì •
        y_pred_yd = bias_correction(y_pred_yd, y_train.values, capacity_yd)
        y_pred_yd = smooth_predictions(y_pred_yd, window=5)

        result_yd = pd.DataFrame({
            'time': yd.loc[test_mask, 'time'].values,
            'energy_kwh': y_pred_yd,
            'plant_name': 'ì˜ë•í’ë ¥'
        })
        results_list.append(result_yd)
        print("ì˜ë• ëª¨ë¸ ì™„ë£Œ")

        # ========== 3. ì–‘ì–‘ ëª¨ë¸ (ë¶„ë¦¬ ëª¨ë¸ ì ìš©) ==========
        print("\nì–‘ì–‘ ëª¨ë¸ í•™ìŠµ ì¤‘ (ë¶„ë¦¬ ëª¨ë¸)...")
        train_mask = yy['month'] % 2 == 0
        test_mask = yy['month'] % 2 == 1

        X_train = yy[train_mask].drop(['time', 'energy_kwh'], axis=1)  # DataFrame ìœ ì§€
        X_test = yy[test_mask].drop(['time', 'energy_kwh'], axis=1)  # DataFrame ìœ ì§€
        y_train = yy.loc[train_mask, 'energy_kwh']  # Series ìœ ì§€

        capacity_yy = CAPACITY['ì–‘ì–‘í’ë ¥']

        # ë¶„ë¦¬ ëª¨ë¸ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡
        # y_pred_yy, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_yy, model_type='lgbm')
        y_pred_yy, model_high, model_low = train_dual_model(X_train, y_train, X_test, capacity_yy, model_type='elasticnet')

        # ë°”ì´ì–´ìŠ¤ ë³´ì •
        y_pred_yy = bias_correction(y_pred_yy, y_train.values, capacity_yy)
        y_pred_yy = smooth_predictions(y_pred_yy, window=5)
        result_yy = pd.DataFrame({
            'time': yy.loc[test_mask, 'time'].values,
            'energy_kwh': y_pred_yy,
            'plant_name': 'ì–‘ì–‘í’ë ¥'
        })
        results_list.append(result_yy)
        print("ì–‘ì–‘ ëª¨ë¸ ì™„ë£Œ")

        print("\n=== ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ===")
        return results_list



    def save_results(self, results_list):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        print("=== ê²°ê³¼ ì €ì¥ ===")

        final_df = pd.concat(results_list, ignore_index=True)
        result_file_path = self.result_path / 'result.csv'
        final_df.to_csv(result_file_path, index=False)

        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file_path}")
        print(f"ì´ ì˜ˆì¸¡ ë°ì´í„° ìˆ˜: {len(final_df)}")
        return final_df

    def load_processed_data(self):
        """ì´ë¯¸ íŒŒìƒë³€ìˆ˜ê°€ ì¶”ê°€ëœ ìµœì¢… ë°ì´í„° ë¡œë“œ"""
        print("=== ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ===")

        processed_files = {
            'gy': self.derived_data_path / 'gy.parquet',
            'yy': self.derived_data_path / 'yy.parquet',
            'yd': self.derived_data_path / 'yd.parquet'
        }

        # ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        all_exist = all(path.exists() for path in processed_files.values())

        if all_exist:
            gy = pd.read_parquet(processed_files['gy'])
            yy = pd.read_parquet(processed_files['yy'])
            yd = pd.read_parquet(processed_files['yd'])
            print("ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return gy, yy, yd
        else:
            missing = [name for name, path in processed_files.items() if not path.exists()]
            print(f"ìµœì¢… ì²˜ë¦¬ ë°ì´í„° ì—†ìŒ: {missing}")
            return None, None, None

    def run_full_pipeline(self, skip_weather_processing=True, use_processed_data=True):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=== í’ë ¥ ë°œì „ëŸ‰ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {self.base_path}")

        # 1. ì´ë¯¸ ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if use_processed_data:
            gy, yy, yd = self.load_processed_data()
            if gy is not None and yy is not None and yd is not None:
                print("ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš© - ë°”ë¡œ ëª¨ë¸ë§ ë‹¨ê³„ë¡œ ì§„í–‰")
                # 5. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
                results_list = self.train_models_and_predict(gy, yd, yy)
                # 6. ê²°ê³¼ ì €ì¥
                final_df = self.save_results(results_list)
                print("=== íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
                return final_df

        # 2. ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬ (ì„ íƒì )
        if not skip_weather_processing:
            self.process_weather_data()

        # 3. ì›ë³¸ ë‚ ì”¨ ë°ì´í„° ë¡œë“œ (MAX ë°ì´í„°)
        df1, df2, df3 = self.load_weather_data()
        target_data = self.load_target_data()

        # 4. íŒŒìƒë³€ìˆ˜ ìƒì„± ë° ë°ì´í„° ê²°í•©
        gy, yd, yy = self.create_features(df1, df2, df3, target_data)

        # 5. ë°ì´í„° ì „ì²˜ë¦¬
        gy, yd, yy = self.preprocess_data(gy, yd, yy)

        # 6. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
        results_list = self.train_models_and_predict(gy, yd, yy)

        # 7. ê²°ê³¼ ì €ì¥
        final_df = self.save_results(results_list)

        print("=== íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
        return final_df


# WeatherDataProcessor í´ë˜ìŠ¤ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
class WeatherDataProcessor:
    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers
        self.schema_map = {}

    def _get_location_configs(self) -> Dict[str, LocationConfig]:
        return {
            'gyeongju': LocationConfig(
                name='ê²½ì£¼',
                lat=35.7149,
                lon=129.3693,
                lat_range=0.015,
                lon_range=0.015,
                start_date='2020-01-01',
                end_date='2024-12-31'
            ),
            'yangyang': LocationConfig(
                name='ì–‘ì–‘',
                lat=37.9330943,
                lon=128.6943946,
                lat_range=0.02,
                lon_range=0.02,
                start_date='2024-04-01',
                end_date='2025-03-31'
            ),
            'yeongdeok': LocationConfig(
                name='ì˜ë•',
                lat=36.4198685,
                lon=129.3960048,
                lat_range=0.015,
                lon_range=0.015,
                start_date='2024-04-01',
                end_date='2025-03-31'
            )
        }

    def _date_in_range(self, date_str: str, start: str, end: str) -> bool:
        date = dt.datetime.strptime(date_str, "%Y%m%d").date()
        return dt.date.fromisoformat(start) <= date <= dt.date.fromisoformat(end)

    def _filter_lat_lon_lazy(self, df: pl.LazyFrame, config: LocationConfig) -> pl.LazyFrame:
        lat_min = config.lat - config.lat_range
        lat_max = config.lat + config.lat_range
        lon_min = config.lon - config.lon_range
        lon_max = config.lon + config.lon_range

        return df.filter(
            (pl.col("latitude") >= lat_min) & (pl.col("latitude") <= lat_max) &
            (pl.col("longitude") >= lon_min) & (pl.col("longitude") <= lon_max)
        )

    def _get_target_dates(self, root_folder: str, output_folder: str, config: LocationConfig) -> List[str]:
        all_dates = [d for d in os.listdir(root_folder)
                     if os.path.isdir(os.path.join(root_folder, d)) and d.isdigit()]
        done_dates = [d for d in os.listdir(output_folder)
                      if os.path.isdir(os.path.join(output_folder, d)) and d.isdigit()] if os.path.exists(
            output_folder) else []
        target_dates = sorted([d for d in all_dates if d not in done_dates])

        return [d for d in target_dates
                if self._date_in_range(d, config.start_date, config.end_date)]

    def _collect_parquet_files(self, root_folder: str, target_dates: List[str]) -> List[str]:
        all_parquet_files = []
        for date_folder in tqdm(target_dates, desc="Scanning folders", unit="folder"):
            date_path = os.path.join(root_folder, date_folder)
            for root, dirs, files in os.walk(date_path):
                for file in files:
                    if file.endswith(".parquet"):
                        folder_date = date_folder
                        file_date = file.split('_')[0].replace('-', '')
                        if file_date == folder_date:
                            all_parquet_files.append(os.path.join(root, file))
        return all_parquet_files

    def _process_single_file(self, file_path: str, root_folder: str,
                             output_folder: str, config: LocationConfig) -> str:
        lazy_df = pl.scan_parquet(file_path)
        filtered_df = self._filter_lat_lon_lazy(lazy_df, config).collect()
        relative_path = os.path.relpath(file_path, root_folder)
        save_path = os.path.join(output_folder, relative_path)
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        filtered_df.write_parquet(save_path)
        return file_path

    def _process_file_for_timeseries(self, file_path: str) -> Optional[pl.DataFrame]:
        try:
            file_name = os.path.basename(file_path).replace('.parquet', '')
            time_val = datetime.strptime(file_name, "%Y-%m-%d_%H_%M_%S")

            lazy_df = pl.scan_parquet(file_path)
            schema = lazy_df.collect_schema()
            schema_signature = str([(c, str(dtype)) for c, dtype in schema.items()])
            self.schema_map.setdefault(schema_signature, []).append(file_path)

            casted_df = lazy_df.with_columns([
                pl.col(c).cast(pl.Float32) for c, dtype in schema.items()
                if dtype in (pl.Float64,)
            ])
            num_cols = [c for c, dtype in casted_df.collect_schema().items()
                        if dtype in (pl.Float64, pl.Int64, pl.Float32)]
            max_df = casted_df.select([pl.col(c).max().alias(c) for c in num_cols])
            max_df = max_df.with_columns(pl.lit(time_val).alias("time"))

            return max_df

        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {file_path} -> {e}")
            return None

    def _filter_files_by_date_range(self, root_folder: str, start_date: str, end_date: str) -> List[str]:
        all_files = []
        date_folders = sorted(os.listdir(root_folder))
        for date_folder in tqdm(date_folders, desc="Collecting files"):
            date_path = os.path.join(root_folder, date_folder)
            if not os.path.isdir(date_path) or not date_folder.isdigit():
                continue
            if not self._date_in_range_yyyymmdd(date_folder, start_date, end_date):
                continue
            for root, _, files in os.walk(date_path):
                for f in files:
                    if f.endswith(".parquet"):
                        all_files.append(os.path.join(root, f))
        return all_files

    def _date_in_range_yyyymmdd(self, date_str: str, start: str, end: str) -> bool:
        d = datetime.strptime(date_str, "%Y%m%d").date()
        return (datetime.strptime(start, "%Y%m%d").date() <= d <=
                datetime.strptime(end, "%Y%m%d").date())

    def _print_schema_info(self):
        print("\n=== ìŠ¤í‚¤ë§ˆ ê·¸ë£¹ë³„ íŒŒì¼ ë¶„í¬ ===")
        for i, (sig, files) in enumerate(self.schema_map.items(), 1):
            print(f"\n[ìŠ¤í‚¤ë§ˆ {i}] ({len(files)}ê°œ íŒŒì¼)")
            print("ìƒ˜í”Œ ìŠ¤í‚¤ë§ˆ:", sig[:200] + "..." if len(sig) > 200 else sig)
            if len(files) <= 5:
                for f in files:
                    print("  ", f)
            else:
                print("  ... (ì´", len(files), "ê°œ)")

    def process_location_data(self, location_key: str, root_folder: str,
                              output_folder: str, output_parquet: str, output_csv: str):
        configs = self._get_location_configs()
        if location_key not in configs:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€ì—­: {location_key}")

        config = configs[location_key]
        print(f"\n=== {config.name} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
        print("root_folder ì „ì²´ ë‚´ìš©:", os.listdir(root_folder))
        os.makedirs(output_folder, exist_ok=True)

        target_dates = self._get_target_dates(root_folder, output_folder, config)
        all_parquet_files = self._collect_parquet_files(root_folder, target_dates)
        print(f"ì´ {len(all_parquet_files)}ê°œ íŒŒì¼ì„ ì²˜ë¦¬")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            list(tqdm(
                executor.map(
                    lambda fp: self._process_single_file(fp, root_folder, output_folder, config),
                    all_parquet_files
                ),
                total=len(all_parquet_files),
                desc="Filtering files"
            ))

        print(f"\n=== {config.name} ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ===")
        start_date_yyyymmdd = config.start_date.replace('-', '')
        end_date_yyyymmdd = config.end_date.replace('-', '')

        all_files = self._filter_files_by_date_range(
            output_folder, start_date_yyyymmdd, end_date_yyyymmdd
        )
        print(f"ì´ {len(all_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •.")

        lazy_results = []
        with ThreadPoolExecutor(max_workers=min(self.max_workers, os.cpu_count())) as executor:
            futures = {executor.submit(self._process_file_for_timeseries, fp): fp for fp in all_files}
            for future in tqdm(as_completed(futures), total=len(futures), desc="Processing files"):
                res = future.result()
                if res is not None:
                    lazy_results.append(res)

        self._print_schema_info()
        if lazy_results:
            final_lazy_df = pl.concat(lazy_results, how="vertical").sort("time")
            final_df = final_lazy_df.collect()

            final_df.write_parquet(output_parquet)
            final_df.write_csv(output_csv)

            print(f"\nì™„ë£Œ: {len(final_df)}ê°œ ì‹œê°„ëŒ€ ë°ì´í„° ì €ì¥")
            print(f"- Parquet: {output_parquet}")
            print(f"- CSV: {output_csv}")
        else:
            print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    ì‚¬ìš©ìê°€ ê¸°ë³¸ ê²½ë¡œë§Œ ì„¤ì •í•˜ë©´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """

    # ê¸°ë³¸ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
    BASE_PATH = r"C:\Users\dbk32\OneDrive\ë°”íƒ• í™”ë©´\test" 

    # íŒŒì´í”„ë¼ì¸ ê°ì²´ ìƒì„±
    pipeline = WindPowerPipeline(base_path=BASE_PATH, max_workers=8)

    print("=== ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸ ===")
    print(f"ê¸°ë³¸ ê²½ë¡œ: {pipeline.base_path}")
    print(f"LDAPS ë°ì´í„°: {pipeline.ldaps_path}")
    print(f"íƒ€ê²Ÿ ë°ì´í„°: {pipeline.target_path}")
    print(f"ê²°ê³¼ ì €ì¥: {pipeline.result_path}")

    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    required_paths = {
        'ldaps': pipeline.ldaps_path,
        'target': pipeline.target_path,
        'íŒŒìƒë³€ìˆ˜ ë°ì´í„°': pipeline.derived_data_path
    }

    missing_paths = []
    for name, path in required_paths.items():
        if path.exists():
            print(f"âœ“ {name}: {path}")
        else:
            print(f"âœ— {name}: {path} (ì—†ìŒ)")
            missing_paths.append(name)

    if missing_paths:
        print(f"\nê²½ê³ : ë‹¤ìŒ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_paths)}")
        print("íŒŒìƒë³€ìˆ˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì „ì²˜ë¦¬ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

    try:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        # use_processed_data=True: ì´ë¯¸ íŒŒìƒë³€ìˆ˜ê°€ ì¶”ê°€ëœ gy.parquet, yy.parquet, yd.parquet ì‚¬ìš©
        # skip_weather_processing=True: ì´ë¯¸ ì „ì²˜ë¦¬ëœ MAX ë°ì´í„° ì‚¬ìš©
        # skip_weather_processing=False: ì²˜ìŒë¶€í„° ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘

        # ìš°ì„ ìˆœìœ„:
        # 1ìˆœìœ„ - ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„° (gy.parquet, yy.parquet, yd.parquet)
        # 2ìˆœìœ„ - MAX ë°ì´í„°ì—ì„œ íŒŒìƒë³€ìˆ˜ ìƒì„±
        # 3ìˆœìœ„ - ì›ë³¸ ë°ì´í„°ì—ì„œ ì „ì²´ ì „ì²˜ë¦¬

        use_processed = pipeline.derived_data_path.exists()
        skip_weather = not use_processed and any([
            (pipeline.result_path / "ê²½ì£¼_test/ê²½ì£¼_timeseries_MAX.parquet").exists(),
            (pipeline.result_path / "ì–‘ì–‘_test/ì–‘ì–‘_timeseries_MAX.parquet").exists(),
            (pipeline.result_path / "ì˜ë•_test/ì˜ë•_timeseries_MAX.parquet").exists()
        ])

        final_result = pipeline.run_full_pipeline(
            skip_weather_processing=skip_weather,
            use_processed_data=use_processed
        )

        print("\n=== ìµœì¢… ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì´ ì˜ˆì¸¡ ê²°ê³¼: {len(final_result)}ê°œ")
        print("\nì§€ì—­ë³„ ì˜ˆì¸¡ ê°œìˆ˜:")
        for plant in final_result['plant_name'].unique():
            count = len(final_result[final_result['plant_name'] == plant])
            print(f"  {plant}: {count}ê°œ")

        print(f"\nê²°ê³¼ íŒŒì¼: {pipeline.result_path / 'result.csv'}")

    except Exception as e:
        print(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

# ê¸°ë³¸ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
BASE_PATH = r"C:\Users\dbk32\OneDrive\ë°”íƒ• í™”ë©´\test"

## Case 1
# pipeline = WindPowerPipeline(base_path=BASE_PATH, max_workers=8)
#
# # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë“  ë‹¨ê³„ í¬í•¨)
# final_result = pipeline.run_full_pipeline(
#     skip_weather_processing=False,  # ì›ë³¸ë¶€í„° ì „ì²˜ë¦¬
#     use_processed_data=False        # ì¤‘ê°„ ë°ì´í„° ì‚¬ìš© ì•ˆí•¨
# )


## Case 2
# pipeline = WindPowerPipeline(base_path=BASE_PATH)
# final_result = pipeline.run_full_pipeline(
#     skip_weather_processing=True,   # MAX ë°ì´í„° ì‚¬ìš©
#     use_processed_data=False
# )

## Case 3

pipeline = WindPowerPipeline(base_path=BASE_PATH, max_workers=8)

# ìµœì¢… ë°ì´í„°ë¡œ ë°”ë¡œ ëª¨ë¸ë§
final_result = pipeline.run_full_pipeline(
    use_processed_data=True  # ìµœì¢… ë°ì´í„° ì§ì ‘ ì‚¬ìš©
)
